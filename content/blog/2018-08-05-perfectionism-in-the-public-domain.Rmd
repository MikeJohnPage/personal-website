---
title: Perfectionism in the Public Domaim
author: Mike Page
date: '2018-08-05'
slug: perfectionism-in-the-public-domain
categories: []
tags: []
description: Desc
hacker_news_id: ''
lobsters_id: ''
meta_img: /images/image.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center")
```

<span style="color:grey">A blog post aimed at the non-technical reader</span>
<br/>  

### 1. INTRODUCTION
***

##### 1.1 Introduction to the problem

Recent evidence demonstrates that levels of perfectionism in Western populations has linearly increased over the past three decades (Curran & Hill, 2017). This has coincided with a rapid growth in the number of research articles investigating the outcomes, processes, and characteristics associated with perfectionism. Despite a growing body of perfectionism literature in the academic domain, little is known (from the perspective of the academic) regarding reporting standards of perfectionism in the public domain. Indeed, it is important that academic research is *accurately* translated and disseminated to the broader public. Nonetheless, the extent to which this holds true in the realm of perfectionism is unknown.

##### 1.2 What is perfectionism?

Broadly defined, perfectionism is understood to be a multidimensional personality trait consisting of two higher-order dimensions: perfectionistic strivings and perfectionistic concerns (Stoeber & Otto, 2006). Perfectionistic strivings capture the setting of high performance standards and self- oriented strivings for perfection, whereas perfectionistic concerns capture the negative reactions to imperfections and mistakes, and the fear of negative social appraisal (Gotwals, Stoeber, Dunn, and Stoll, 2012). These two dimensions are considered to be part of a hierarchical model or heuristic representative of a range of different models that exists (Hill, 2016). Multiple reviews support that both perfectionistic strivings and perfectionistic concerns display a typical pattern of findings: perfectionistic strivings are associated with adaptive outcomes, processes and characteristics, whereas perfectionistic concerns are associated with maladaptive outcomes, processes, and characteristics (e.g., Gotwals et al., 2012, Stoeber, 2011, Stoeber & Otto, 2006). One may therefore expect public reporting of perfectionism to reflect this patterns of findings.

##### 1.3 Framing the problem

One method to understand the meaning and social understanding of perfectionism, both in regards to whether it is perceived as a positive and/or negative trait (in line with the research on perfectionistic strivings and perfectionistic concerns), and inferred by the other words/topics it coalesces with, is to use something called natural language processing. Simply put, natural language processing is the process of using computers to analyse large amounts of natural language data using a host of statistical techniques. Accordingly, the purpose of this blog post is to dissect and apply some of these natural language processes to better understand how perfectionism is reported in the public domain.

### 2. THE DATA SET
***

##### 2.1 The data

In order to analyse public perceptions of perfectionism, a data set is required! Accordingly, an API client (a program which talks to other computer programs) was built and used to retrieve news articles from across the web using the [NewsRiver API](https://newsriver.io). The returned articles were cleaned (i.e., unwanted data and errors were removed) and the final data set contained four columns: article title (title), date of publication (date), the website the article was retrieved from (website), and the words from each article (word). A subset of the data can be seen in the table below:

```{r data set}
tidy_news <- readr::read_rds("/Users/mikepage/Documents/Data Science/Springboard/Capstone_Project/Capstone Project/tidy_news.RDS")

knitr::kable(tidy_news[17585:17595,])
```

It should be noted that each row only contains one word. This kind of data format is called a 'tidy' data format, where each  each variable is a column, each observation is a row, and each type of observational unit is a table (Wickham, 2014). This simply allows for easier analysis of the data when applying natural language processing techniques.

##### 2.2 Summary of the data

The final data set contained 18,160 observations (words), split across 74 news articles from a variety of sources. Each article contained the word 'perfectionism' or 'perfect' in the title and 'perfectionism' in the text on at least one occasion. The date range of the articles was from 2017-11-22 to 2018-07-03 (these limits were imposed by the API). It should be noted that other data structures (other than the 'tidy' data structure above) were used, but it is not necessary to review these data structures to understand the analyses below.

### 3. ANALYSES
***

##### 3.1 Assessing the fit of the data

Before trying to answer questions such as 'is perfectionism reported in a positive or negative fashion?' or 'what words/topics does perfectionism coalesce with?', it is important to establish the structure of the data being used. Assessing the structure of the data ensures that the data is formatted in an appropriate way so that it can be used to *accurately* answer such questions. A data set not fit for purpose will heavily bias the results.

To assess the fit of the data, first, a plot of term frequency was created. Term frequency measures how frequently a term appears in a document:


```{r preparatory code, include = FALSE}

# Load libraries

library(tidyverse)
library(httr)
library(jsonlite)
library(xml2)
library(urltools)
library(lubridate)
library(magrittr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
library(ldatuning)

# Load data sets

perf_news <- read_rds("/Users/mikepage/Documents/Data Science/Springboard/Capstone_Project/Capstone Project/perf_news.RDS")
tidy_news <- read_rds("/Users/mikepage/Documents/Data Science/Springboard/Capstone_Project/Capstone Project/tidy_news.RDS")

```

```{r term frequency distribution}

words_news <- perf_news %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  ungroup()

words_total <- words_news %>% 
  group_by(title) %>% 
  summarise(total = sum(n))

words_news <- left_join(words_news, words_total)

ggplot(words_news, aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.025) +
  theme(strip.text = element_text(size = 7))

```

The above plot shows how frequently a word is likely to appear in relation to the total number of words in each article. As would be expected, a long tail to the right can be observed. This simply means that some types of words such as articles (e.g., 'the') appear more frequently than other types of words. 

To further explore the term frequency distribution shown above, a second plot examining frequency rank (rank of each word by frequency) against term frequency was then created. This kind of plot indicates which kind of words in the documents deviate from a typical corpus (collection of texts) of natural language:

```{r linear model, include = FALSE}

freq_by_rank <- words_news %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), `term frequency` = n/total)

lm(log10(`term frequency`) ~ log10(rank), data = freq_by_rank)

```

```{r Zipfs law}

 freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_abline(intercept = -1.1029,slope = -0.7564, color = "black", linetype = 2) +
  geom_line(size = 0.5, alpha = 0.5, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

```

In a 'perfect' corpus of text, it would be expected that the model plotted above would fall directly on the linear model imposed on top of the plot (the black dotted line). However, in this instance, there are small deviations at both the higher and lower ranked words (as indicated by the deviations away from the black dotted line). This means that the data set contains fewer rare and common words than would typically be predicted by a mathematical model. Nonetheless, these kind of deviations are not uncommon for many kinds of natural language (Silge & Robinson (2017). Therefore, the plots in Figure 1 and Figure 2 confirm that data in this project represent a typical corpus of natural language.

##### 3.2 Word frequencies

To understand the basic text composition of the corpus of news articles, a list of custom stop words was removed from the data (words such as 'the' and 'if' which have little, if not no meaning in the text) and the most common words were found:

```{r word frequencies bar}
 
# Create custom stop words

custom_stop_words <-  bind_rows(tibble(word = c("perfect", 
                                                "perfection", 
                                                "perfectionism", 
                                                "perfectly", 
                                                "perfectionist", 
                                                "perfectionists", 
                                                "curran", 
                                                "thomas", 
                                                "andy", 
                                                "hill"), 
                                       lexicon = c("custom")), stop_words)

# Find and plot the most common words in tidy_news after applying custom stop words

# As a bar chart:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Word Frequency (n)")

```

The most common words match those one would expect to find in a corpus of language discussing perfectionism. For example, words such as 'expectations', 'standards', and 'pressure' appear, words commonly used in the academic domain that reflect the high expectations perfectionists place upon themselves. Other words such as 'students', 'college', and 'university' also appear, again, unsurprising given that a large body of academic research has been conducted in University samples. This basic frequency analysis indicates that the words used in this corpus of text can be paralleled to that in the academic domain. The word cloud below allows the reader to further explore the most frequently used words:

```{r word frequencies bar cloud}

# As a word cloud:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100, colors= c("steelblue1","steelblue2","steelblue3","steelblue")))

```

##### 3.3 Sentiment Analyses

Having established that the data represented a typical corpus of natural language, and contained words inline with those one may expect in a corpus of text examining perfectionism, a sentiment analysis over time was performed. This was done in order to explore the underlying polarity of the data (i.e., if perfectionism was reported in a positive or negative fashion), and to see if this has changed over time. While a variety of approaches exist to examine text sentiment, in general, the process involves scoring each word in the data set a sentiment value (this can take many forms from a numeric score (e.g., -5, 2, 3) to a word (e.g. positive, negative, anger)). These values are derived from pre-compiled lists called sentiment lexicons. For this project, three commonly used sentiment lexicons were used: Bing, AFINN, and NRC. For each lexicon, sentiment was split into negative and positive classifications, inline with the theory on perfectionistic strivings and perfectionistic concerns:


```{r tidier news, include = FALSE}

# Create a new data set called tidier_news which is tokenized by word, but keeps track of sentence number

tidier_news <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  anti_join(custom_stop_words)

```


```{r sentiment analyses}

# Perform sentiment analyses using three different sentiment lexicons (AFINN, Bing, and NRC). Compute sentiment in sentence units by summing individual word sentiment scores across each sentence.

# AFINN

afinn <- tidier_news %>% 
  inner_join(get_sentiments("afinn")) %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Bing

bing <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# NRC

nrc <- tidier_news %>% 
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Plot all three sentiment analyses on one graph

bind_rows(afinn %>% mutate(method = "AFINN"), bing %>% mutate(method = "Bing et al."), nrc %>% mutate(method = "NRC")) %>% 
  ggplot(aes(date, sent_sum, fill = method)) +
  geom_col(position = position_dodge(0.5), show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  labs(x = "Date", y = "Sentiment Score")

```

The data above reveals several noteworthy artifacts about the data. Firstly, the frequency of publications over time remains consistent, with no time period demonstrating a noticeable increase in publication frequency. Secondly, there appears to be no significant trends in sentiment over time (i.e., the distribution of sentiment scores remains consistent over the time). Finally, the plots reveal that articles report perfectionism in both a positive and negative manner. Using the AFINN and Bing sentiments, perfectionism is frequently reported in a negative fashion, whereas the NRC lexicon demonstrates the opposite effect. One reason for the observed difference in sentiment scores between lexicons may be the higher ratio of positive to negative words used in the NRC lexicon in comparison to the AFINN and Bing lexicons. The table below demonstrates this difference between the Bing and NRC lexicons:


```{r lexicon comparison}

nrc_lex <- get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment) %>% 
  mutate(lexicon = "nrc")

bing_lex <- get_sentiments("bing") %>% 
  count(sentiment) %>% 
  mutate(lexicon = "bing")

knitr::kable(bind_rows(nrc_lex, bing_lex))
```

The most frequent positive and negative words across all texts were then found using the Bing sentiment lexicon: 

```{r positive negative bar}

# Find the most common positive and negative words

bing_word_counts <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

# Plot the most common positive and negative words

# As a bar chart:

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "contribution to sentiment", x = NULL) +
  coord_flip()

```

Similar to the term frequency analysis conducted earlier, many predictable terms appear. For example, of the most frequent negative terms, 'failure', 'mistakes', and 'unrealistic' are commonly used in the academic literature to denote the unrealistic expectations perfectionists demand upon themselves, and their associated fear of failure and mistakes. This also extends to terms such as 'hard', 'unhealthy', and 'depression', terms in the academic literature which denote the experiences perfectionists typically find themselves in. On the contrary, of the most frequent positive terms, many terms appear unexpected. Indeed some terms such as 'positive' and 'healthy' do parallel those terms used in the academic literature (denoting perfectionistic strivings), however, terms such as 'love', 'worth', and 'happy' are not common occurrences in academic texts. Moreover, one would expect literature discussing perfectionism to discuss low self-worth, and a deficit of happiness and love reflecting the relationship and life difficulties associated with perfectionism (Dunkley et al., 2003;  Hewitt and Flett, 2002). Perhaps one explanation for the occurrence of unexpected words is the use of unigrams to determine sentiment. Indeed, statements such as 'not happy' reflect a negative sentiment, yet result in a positive sentiment as the outcome (i.e, not gets discarded as a stop word and happy gets treated as a positive sentiment) using the methods employed above. One way to overcome and further explore this is through the use of bigrams.

##### 3.4 Bigrams

```{r bigram prep, include = FALSE}

# Unnest tokens by bigrams keeping track of sentence number

bigram_news <- perf_news %>%
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)

# Separate bigrams into two columns, "word1", and "word2".

bigrams_separated <- bigram_news %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

```

In order to account for any negation words in the data set and their effect on sentiment scores, a further sentiment analysis using bigrams was performed. Whereas previously single words (i.e, unigrams) were used to analyse the data, now bigrams were used, which is when the text is split into pairs of consecutive words. To perform the sentiment analysis on bigrams, the sentiment score of any bigrams that matched a list of negation words ('not', 'never', 'no', 'without') were reversed:

```{r bigram sentiment}

# Use bigrams to perform sentiment analyses by reversing the sentiment score of negated words

AFINN <- get_sentiments("afinn")

negation_words <- c("not", "never", "no", "without")

bigrams_afinn <- bigrams_separated %>% 
  filter(!word1 %in% custom_stop_words$word) %>% 
  filter(!word2 %in% custom_stop_words$word) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>%
  mutate(score = ifelse(word1 %in% negation_words, -score, score))

bigrams_afinn_sentiment <- bigrams_afinn %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

ggplot(bigrams_afinn_sentiment, aes(date, sent_sum, fill = date)) +
  geom_col(position = position_dodge(0.7), width = 2, show.legend = FALSE) +
  labs(y = "sentiment score", x = "date")

```

The data above demonstrate that after controlling for negation words, the articles still maintained a split across both positive and negative sentiments. However, in this instance, there was a heavier weighting towards a negative sentiment (as would be expected after controlling for negation words).

A frequency count of the most common bigrams was then computed to further explore the underlying structure of the data, as can be seen in the table below:

```{r bigram frequency}

# Count most frequent bigrams not keeping track of sentence

bigram_counts <- perf_news %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)

knitr::kable(bigram_counts[1:10, ])


```

The data in the above table demonstrates that the public reporting of perfectionism is inline with the latest research in the academic domain. For instance, the most frequent bigram terms closely match those of a recent publication by Curran and Hill (2017) published in Psychological Bulletin in which 'social media', 'socially prescribed (perfectionism)' and 'mental health' are discussed in the context of 'college students'.

##### 4.5 Pairwise correlations

To determine which words various perfectionism terms (e.g., perfect, perfectionist, etc.) coalesce with, pairwise correlations were calculated. This identified how much more likely it was that a word would appear in conjunction with a perfectionism term, or that neither term appeared, than that one appeared without the other:

```{r pairwise prep, include = FALSE}

# Calculate pairwise correlations of words using the phi coefficient. This identifies how much more likely it is that either both word X and Y appear, or neither do, than that one appears without the other.

word_cors <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  group_by(word) %>% 
  filter(n() >= 20) %>% 
  pairwise_cor(word, title, sort = TRUE)

```

```{r pairwise cor}

# Plot words most correlated with different perfectionism terms

word_cors %>%
  filter(item1 %in% c("perfect", "perfection", "perfectionism", "perfectly", "perfectionist", "perfectionists")) %>% 
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>% 
  group_by(item1, item2) %>%                  
  arrange(desc(correlation)) %>%                
  ungroup() %>%
  mutate(item2 = factor(paste(item2, item1, sep = "__"), levels = rev(paste(item2, item1, sep = "__")))) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)

```

The words correlated with perfectionism terms mirrors those found in the frequency analysis conducted previously. These words include 'mistakes', 'fear', 'failure', and 'expectations', all words in the arsenal of the academic perfectionism lexicon. Moreover, the terms correlated with 'perfect' and 'perfectionists' are in keeping with this theme, albeit they are less frequently employed terms in the academic literature. For instance, the occurrence of the terms 'pregnancy', 'person', 'kids', and 'week' are not published about frequently in the academic domain, but indeed, a small handful of articles do exists examining these topics (e.g., Macedo et al., 2009).

##### 4.6 Topic models

One question of interest is what topics perfectionism coalesces with in the collected news articles. In order to answer such a question the data must be divided into a set of natural groups. One such method for achieving this is topic modelling, a method of unsupervised classification of documents. In topic modelling, each documented is treated as a mixture of topics, and each topic as a mixture of words. This prevents documents being categorised into discrete groups, and allows for overlap in terms of content in a way that represents the structure of natural language. Consequently, topic models were fitted to the data, and nine topics were found:
```{r dtm, include = FALSE, cache = TRUE}

# Create Document Term Matrix

news_dtm <- tidy_news %>% 
  anti_join(custom_stop_words) %>%
  count(title, word) %>% 
  cast_dtm(title, word, n)

# Select number of topics (k) for LDA model using the 'ldatuninig' package.

lda_fit <-FindTopicsNumber(news_dtm,
                           topics = seq(from = 2, to = 50, by = 1),
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
                           method = "Gibbs", control = list(seed = 77), mc.cores = 2L, verbose = TRUE)

```

```{r fit topic models, include = FALSE}

# Fit topic models using latent Dirichlet allocation

perf_lda <- LDA(news_dtm, k = 9, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (beta).

perf_topics <- tidy(perf_lda, matrix = "beta")

```

Having established the number of topics (9), the most common words within each topic were calculated:

```{r topic model terms}

# Find most common terms for each topic.

perf_top_terms <-  perf_topics %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

perf_top_terms %>% 
  group_by(topic, term) %>%                  
  arrange(desc(beta)) %>%                
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), levels = rev(paste(term, topic, sep = "__")))) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)
```

As can be seen above, some topics overlap. For example, topics three, five, and seven contain at least one of the terms 'life' and 'people' in the top two terms, whereas topics four, seven, and nine contain the terms 'kids' or 'parents'. It can be observed that there are meaningful differences between the nine topics, with some of these topics in keeping with the perfectionism themes highlighted so far. For example topic two is unique in its use of terms such as 'college' and 'students' and reflects the academic research of Curran and Hill (2017) highlighted previously. Other topics are also unique, such as topics four and seven, but their relevance to perfectionism is not as clear. For example, topic one contains key terms such as 'code', 'music', and 'time', topics not frequently employed in the academic literature. That is not to say that all discussions of perfectionism in the public domain should exactly mirror those in the academic domain, rather, it is interesting to note the diversity of topics discussed outside of the academic setting, as exemplified in this topic model.

### 5. SUMMARY
***

Three key themes emerged in the data:

1. The most frequently used words across all articles closely mirror those used in the academic literature. This is true across the unigram and bigram term frequencies. There is a general discussion around the themes of 'pressure' and 'standards' in 'universities' and 'college', all themes researched in great depth in the academic setting. This preliminary indicates that perfectionism is, at least in part, being discussed in the public domain in a manner that reflects the broader body of academic research.

2. The sentiment analyses revealed two important artifacts of the data. Firstly, perfectionism is discussed in both a positive and negative fashion. The dichotomy of positive and negative aspects of perfectionism closely mirrors the large body of research in the academic domain investigating perfectionistic strivings and perfectionistic concerns respectively. Secondly, in general, there is a heavier weighting towards the more negative aspects of perfectionism. Interestingly, this pattern of findings also mirrors that in the academic domain. While there is evidence to suggest that perfectionism is associated with some positive outcomes, processes and characteristics, there is a larger body of evidence indicating that perfectionism has a greater association with negative outcomes, processes, and characteristics (Hill, 2014). This effect held true even after controlling for negation words (e.g., not, never, etc.).

3. The topic model analysis revealed a large diversity in topics across the articles. This implies that in some instances, perfectionism is being talked about in settings that typically fall outside of academic research. This may suggest that in some instances, perfectionism is being reported in the public domain in a manner not in keeping with the broader scientific evidence. However, this assertion requires careful scrutiny, as the topics require contextualising within each article to understand their significance, and the impact of any misreporting would require further analysis.

### 6. REFERENCES

Curran, T., & Hill, A. P. (2017, December 28). Perfectionism Is Increasing Over Time: A MetaAnalysis of Birth Cohort Differences From 1989 to 2016. *Psychological Bulletin*. Advance online publication. http://dx.doi.org/10.1037/bul0000138.

Dunkley, D. M., Zuroff, D. C., & Blankstein, K. R. (2003). Self-critical *perfectionism and daily affect: dispositional and situational influences on stress and coping. *Journal of Personality and Social Psychology*, 84, 234-252.

Gotwals, J. K., Stoeber, J., Dunn, J. G., & Stoll, O. (2012). Are perfectionistic strivings in sport adaptive? A systematic review of confirmatory, contradictory, and mixed evidence. *Canadian Psychology*, 53, 263-279.

Hewitt, P. L., & Flett, G. L. (2002). Perfectionism and stress processes in psychopathology. In G.L. Flett & P.L. Hewitt (Eds.), *Perfectionism: Theory, Research, and Treatment* (pp. 255-284). Washington, DC: American Psychological Association.

Hill, A. P. (2014). Perfectionistic strivings and the perils of partialling. *International Journal of Sport & Exercise Psychology*, 12, 302-315.

Hill, A.P. (2016). Conceptualizing perfectionism: An overview and unresolved issues. In A.P. Hill (Ed.), *The Psychology of Perfectionism in Sport, Dance and Exercise* (pp. 3-30). London: Routledge.

Macedo, A., Bos, S.C., Marques, M. et al. (2009). Perfectionism dimensions in pregnancy — a study in Portuguese women. *Arch Womens Ment Health*, 12: 43. https://doi.org/10.1007/s00737-008-0042-5

Silge, J, & Robinson, D. (2017). *Text Mining with R: a Tidy Approach*. CA: O'Reilly Media.

Stoeber, J. (2011). The dual nature of perfectionism in sports: Relationships with emotion, motivation, and performance. *International Review of Sport and Exercise Psychology*, 4(2), 128-145.

Stoeber, J., & Otto, K. (2006). Positive conceptions of perfectionism: Approaches, evidence, challenges. *Personality and Social Psychology Review*, 10, 295- 319.

Wickham, H. (2014). Tidy data. *Journal of Statistical Software*, 59.