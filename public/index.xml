<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mike John Page</title>
    <link>/</link>
    <description>Recent content on Mike John Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Tue, 16 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exporting a MySQL Database from an AWS RDS Instance to a Local Instance</title>
      <link>/blog/exporting-a-mysql-database-from-an-aws-rds-instance-to-a-local-instance/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/exporting-a-mysql-database-from-an-aws-rds-instance-to-a-local-instance/</guid>
      <description>Intro In this brief post, you will learn how export a MySQL database hosted on an AWS RDS instance to a local instance on your machine. To do this, you will first update the rules of the security group on your RDS instance hosting your MySQL database, then you will export a dump file locally, and finally you will import the dump file to your local machine.
To follow along with this tutorial, it is assumed you have a MySQL database hosted on an RDS instance and you have MySQL installed on your local machine.</description>
    </item>
    
    <item>
      <title>Returning Tweets on a Schedule in R, using AWS (EC2 &#43; RDS) and Cron</title>
      <link>/blog/returning-tweets-on-a-schedule-in-r-using-aws-ec2-rds-and-cron/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/returning-tweets-on-a-schedule-in-r-using-aws-ec2-rds-and-cron/</guid>
      <description>Intro In this post, you will learn how to run R scripts on a schedule using Amazon Web Services (AWS). Specifically, you learn how to deploy R on an Amazon Elastic Cloud Computing (EC2) instance, how to write data collected in R to an Amazon Relational Database (RDS), and how to schedule tasks on EC2 using cron.
The R code you will be deploying on the EC2 instance is a script that returns Tweets from the Twitter API and then appends them to a MySQL database hosted on an RDS instance.</description>
    </item>
    
    <item>
      <title>Code Snippet: News API Function</title>
      <link>/blog/code-snippet-news-api-function/</link>
      <pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/code-snippet-news-api-function/</guid>
      <description># Load Libraries library(tidyverse) library(httr) library(jsonlite) # news_api function that returns tibble of news for given parameters news_api &amp;lt;- function(query, apiKey, from, to) { # Create a list of query parameters, calls are limited to one month query_params &amp;lt;- list(q = query, language = &amp;quot;en&amp;quot;, from = from, to = to, pageSize = &amp;quot;100&amp;quot;, page = &amp;quot;1&amp;quot;, apiKey = apiKey) # Make a GET request to the News API with the query paramaters response &amp;lt;- GET(&amp;quot;https://newsapi.</description>
    </item>
    
    <item>
      <title>Drawing Insight from Simple Data Exploration and Visualisation, Including Dodged Bars and Choropleth Maps</title>
      <link>/blog/drawing-insight-from-simple-data-exploration-and-visualisation/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/drawing-insight-from-simple-data-exploration-and-visualisation/</guid>
      <description>INTRODUCTION Recently, I got approached by a client starting a company that specialises in company formations and related services. They asked if I could source and analyse data regarding the number of companies formed in the UK each year, in addition to any information on competitor companies in that sector. This information was to be merged into a business plan. I took on the challenge as I reasoned it would be a good opportunity to practice data wrangling and simple data exploration and visualisation.</description>
    </item>
    
    <item>
      <title>Web Scraping, R&#39;s data.table, and Writing to PostgreSQL and MySQL</title>
      <link>/blog/web-scraping-r-s-data-table-and-writing-to-postgresql-and-mysql/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/web-scraping-r-s-data-table-and-writing-to-postgresql-and-mysql/</guid>
      <description>AIMS By the end of this post you should: (i) understand how to use the ‘rvest’ package to scrape web pages, (ii) understand the benefits of using the ‘data.table’ package to wrangle large data sets over other common packages such as ‘dplyr’, (iii) be able to write/read a data set from memory on your local machine to a SQL database (in this instance on a remote server).
To achieve these aims, we are going to scrape movie scripts from IMSDb using ‘rvest’, wrangle the data using the ‘data.</description>
    </item>
    
    <item>
      <title>Bash Aliases</title>
      <link>/blog/bash-alias/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/bash-alias/</guid>
      <description>In this quick post, I will show you how to make a bash alias. To understand what an alias does, Wikipedia provides a good definition:
 In computing, aliasing describes a situation in which a data location in memory can be accessed through different symbolic names in the program.
 In short, creating an alias in bash (the unix shell command language) allows you to type some specific keyword to run whatever you choose to alias (e.</description>
    </item>
    
    <item>
      <title>Rendering JavaScript content using Python, Selenium, and a Headless Browser</title>
      <link>/blog/rendering-javascript-content-using-python-selenium-and-a-headless-browser/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/rendering-javascript-content-using-python-selenium-and-a-headless-browser/</guid>
      <description>This post will demonstrate how to scrape data from the web rendered in JavaScript (JS), using Python and several other tools. To achieve this, we will build a scraper that collects the current dates weather forecast for a given location (rendered in JS) from Wunderground.
First, the robots.txt file on the Wunderground site must be inspected, in addition to the Terms of Service of the site, to see if scraping information is permitted:</description>
    </item>
    
    <item>
      <title>Ubuntu 18.04 LTS &#43; Thinkpad X1 Carbon 6th Generation</title>
      <link>/blog/ubuntu-18-04-lts-thinkpad-x1-carbon-6th-generation/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/ubuntu-18-04-lts-thinkpad-x1-carbon-6th-generation/</guid>
      <description>In this blog post, I will be listing some of the steps I took to set up Ubuntu 18.04 LTS on a Lenovo ThinkPad X1 Carbon 6th Gen. This includes installation steps, as well as post-installations packages I have found that I now use in my daily workflow. This serves to both remind myself of how to set up Ubuntu on future machines in a way I now deem to be near-optimal, and to also help others who may be having difficulty getting Ubuntu to run the way they would like.</description>
    </item>
    
    <item>
      <title>GGDOT Hacknight: using Box-Cox Transformations and Regression to Analyse the CO2 Emissions of Food</title>
      <link>/blog/ggdot-hacknight-using-box-cox-transformations-and-regression-to-analyse-the-co2-emissions-of-food/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/ggdot-hacknight-using-box-cox-transformations-and-regression-to-analyse-the-co2-emissions-of-food/</guid>
      <description>GGDOT HACKNIGHT The GGDOT project combines expertise in greenhouse gas emission calculations, nutrition, and data science to deliver a toolkit. This toolkit contains information on the greenhouse gas emissions and nutritional content of the food we eat. As part of the project, GGDOT host Hacknights to help develop their tools and analyse the data within them. I was fortunate enough to be able to attend the recent Manchester Hacknight, and this blog post will go through the analyses I performed from that session (and the day after).</description>
    </item>
    
    <item>
      <title>Phyllotaxis Project</title>
      <link>/blog/phyllotaxis-project/</link>
      <pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/phyllotaxis-project/</guid>
      <description>Michael Page 
INFO This post isn’t so much a ‘blog’ post as it is a repository for me to store my code for the ‘Phyllotaxis: Draw flowers using mathematics’ project which can be found on DataCamp. Most of the code follows the standard solutions for the project, with the exception of using theme_void() to remove elements from a plot. In this case, using theme_void() is a more elegant solution (in my opinion) than independetly assigning element_blank() to each element (e.</description>
    </item>
    
    <item>
      <title>Multilevel Modelling in SPSS</title>
      <link>/blog/multilevel-modelling-in-spss/</link>
      <pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/multilevel-modelling-in-spss/</guid>
      <description>Michael Page 
INTRODUCTION Nested data structures When selecting an analysis for a given data set it is important to consider if the data is in a nested (i.e., hierarchical/clustered/grouped) structure. A nested data structure is one in which the data is organised at more than one level. For example, students can be nested within classes as so:
 In a similar fashion, it could be said that individual student test scores can be nested within students, who are nested within classes, which are nested within schools:</description>
    </item>
    
    <item>
      <title>Perfectionism in the Public Domain: a Natural Language Processing Approach</title>
      <link>/blog/perfectionism-in-the-public-domain-a-natural-language-processing-approach/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/perfectionism-in-the-public-domain-a-natural-language-processing-approach/</guid>
      <description>Michael Page 
1. INTRODUCTION 1.1 Introduction to the problem Recent meta-analytical evidence demonstrates that levels of perfectionism in Western populations has linearly increased over the past three decades (Curran &amp;amp; Hill, 2017). This has coincided with a rapid growth in the number of research articles investigating the outcomes, processes, and characteristics associated with perfectionism since the introduction of the first multidimensional perfectionism measures in the early 90’s. Despite a growing body of perfectionism literature in the academic domain, little is known (from the perspective of the academic) regarding reporting standards of perfectionism in the public domain.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Sat, 22 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Data scientist, R enthusiast, mountain biker, rock climber, surfer, musician, photographer.
Click here for my CV.</description>
    </item>
    
    <item>
      <title>License</title>
      <link>/license/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/license/</guid>
      <description>The MIT License (MIT)
Copyright &amp;copy; 2019 Michael Page
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &amp;ldquo;Software&amp;rdquo;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</description>
    </item>
    
    <item>
      <title>Packages</title>
      <link>/packages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/packages/</guid>
      <description>I independetly created and maintain the following R packages:
  .columnval { float: left; width: 35%; padding: 20px; }
.rowval::after { clear: both; display: table; }</description>
    </item>
    
  </channel>
</rss>